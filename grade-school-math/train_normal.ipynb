{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained GPTNeoX: EleutherAI/pythia-70m\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n"
     ]
    }
   ],
   "source": [
    "# Init modeo\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('./train_normal.ipynb').resolve().parent.parent))\n",
    "\n",
    "from model import GPT\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = GPT.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer.add_tokens(['<|dense|>'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dense_token_id = tokenizer.encode('<|dense|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 train examples\n",
      "Max tokens: 443\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from grade_school_math.dataset import get_examples, GSMDataset\n",
    "\n",
    "train_examples = get_examples(\"train\")\n",
    "train_dset = GSMDataset(tokenizer, train_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b77a3cb39a946f19775bf3eb4176e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alex/projects/nanoGPT/grade-school-math/train_normal.ipynb Cell 3\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/nanoGPT/grade-school-math/train_normal.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m noop_dense \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_embd))\u001b[39m.\u001b[39mto(device_type)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/nanoGPT/grade-school-math/train_normal.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m logits, dense, loss \u001b[39m=\u001b[39m model(X, noop_dense, Y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alex/projects/nanoGPT/grade-school-math/train_normal.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/nanoGPT/grade-school-math/train_normal.ipynb#W2sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/nanoGPT/grade-school-math/train_normal.ipynb#W2sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device_type = 'cpu'\n",
    "device = torch.device(device_type)\n",
    "# config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "weight_decay = 1e-2\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "learning_rate = 2e-5 # max learning rate\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "pbar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        X = batch['input_ids'][:, :-1].to(device)\n",
    "        Y = batch['input_ids'][:, 1:].to(device)\n",
    "        noop_dense = torch.zeros((X.shape[0], X.shape[1], model.config.n_embd)).to(device_type)\n",
    "\n",
    "        logits, dense, loss = model(X, noop_dense, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        lr_scheduler.step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"train_loss: {loss.item():.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319 test examples\n"
     ]
    }
   ],
   "source": [
    "test_examples = get_examples(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?\\n',\n",
       " 'answer': 'He sprints 3*3=<<3*3=9>>9 times\\nSo he runs 9*60=<<9*60=540>>540 meters\\n#### 540<|endoftext|>'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = test_examples[3]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?\\nThere are 30 passers per sprint.  How many sards does jogging be he is going to run?\\nThere are 30 - 90 = 200 + 90 = any annual jogging pace.\\nBefore running for a week or so spades',\n",
       " '[invalid]',\n",
       " False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EQUALS_TOKENS = set([30, 426, 4010])\n",
    "\n",
    "from grade_school_math.calculator import use_calculator\n",
    "\n",
    "\n",
    "qn = example[\"question\"]\n",
    "\n",
    "for _ in range(50):\n",
    "    with torch.no_grad():\n",
    "        toks = tokenizer([qn], padding=False, return_tensors=\"pt\").to(device)\n",
    "        orig_len = toks[\"input_ids\"].shape[1]\n",
    "        out = model.generate(\n",
    "            toks['input_ids'], max_new_tokens=1\n",
    "        )\n",
    "        text = tokenizer.batch_decode(out)[0]\n",
    "        if out[0, -1].item() in EQUALS_TOKENS:\n",
    "            answer = use_calculator(text)\n",
    "            if answer is not None:\n",
    "                print(\"Triggered calculator, answer\", answer)\n",
    "                text = text + str(answer) + \">>\"\n",
    "\n",
    "        qn = text\n",
    "qn\n",
    "\n",
    "from grade_school_math.dataset import extract_answer, is_correct\n",
    "\n",
    "answer = extract_answer(qn)\n",
    "correct = is_correct(qn, example)\n",
    "qn, answer, correct\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained GPTNeoX: EleutherAI/pythia-70m\n",
      "number of parameters: 70.43M\n"
     ]
    }
   ],
   "source": [
    "# Init modeo\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('./train_normal.ipynb').resolve().parent.parent))\n",
    "\n",
    "from model import GPT\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = GPT.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer.add_tokens(['<|dense|>', '<|pad|>'])\n",
    "tokenizer.pad_token = '<|pad|>'\n",
    "dense_token_id = tokenizer.encode('<|dense|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 train examples\n",
      "Max tokens: 443\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from grade_school_math.dataset import get_examples, GSMDataset\n",
    "\n",
    "train_examples = get_examples(\"train\")\n",
    "train_dset = GSMDataset(tokenizer, train_examples)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, return_tensors='pt')['input_ids']\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids.contiguous(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8063173d52040e9b6543e8e7dc02056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "device_type = 'cpu'\n",
    "dtype = 'bfloat16'\n",
    "device = torch.device(device_type)\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "weight_decay = 1e-2\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "learning_rate = 2e-5 # max learning rate\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optim = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "pbar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        X = batch['input_ids'][:, :-1].to(device)\n",
    "        Y = batch['input_ids'][:, 1:].clone().to(device)\n",
    "        \n",
    "        # Do not compute loss on padding tokens\n",
    "        Y[Y == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        noop_dense = torch.zeros((X.shape[0], X.shape[1], model.config.n_embd)).to(device_type)\n",
    "\n",
    "        with ctx:\n",
    "            logits, dense, loss = model(X, noop_dense, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        lr_scheduler.step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"train_loss: {loss.item():.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319 test examples\n"
     ]
    }
   ],
   "source": [
    "test_examples = get_examples(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.  She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed.  In the afternoon, she gives her chickens another 25 cups of feed.  How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?\\n\",\n",
       " 'answer': 'If each chicken eats 3 cups of feed per day, then for 20 chickens they would need 3*20=<<3*20=60>>60 cups of feed per day.\\nIf she feeds the flock 15 cups of feed in the morning, and 25 cups in the afternoon, then the final meal would require 60-15-25=<<60-15-25=20>>20 cups of chicken feed.\\n#### 20<|endoftext|>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = test_examples[4]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggered calculator, answer 72.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.  She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed.  In the afternoon, she gives her chickens another 25 cups of feed.  How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?\\nEach of the 14 cups of feed (a total of 14+15) brings an average of 1 �¾ cup of food>>.\\nAt a separate meal, Wendi shows 3 cups of feed and 3 eggs throws her chickens back to bed. At 7 on the bed, she gives 6 cups of food.\\nAt 7 on the bed, she gives the chickens 12/5 cup * 30/5 = <<12/5*30=72.0>>72 cups of food.\\nAt 11 on the\",\n",
       " '[invalid]',\n",
       " False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EQUALS_TOKENS = set([30, 426, 4010])\n",
    "\n",
    "from grade_school_math.calculator import use_calculator\n",
    "\n",
    "\n",
    "qn = example[\"question\"]\n",
    "\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "        toks = tokenizer([qn], padding=False, return_tensors=\"pt\").to(device)\n",
    "        orig_len = toks[\"input_ids\"].shape[1]\n",
    "        out = model.generate(\n",
    "            toks['input_ids'], max_new_tokens=1,\n",
    "        )\n",
    "        text = tokenizer.batch_decode(out)[0]\n",
    "        if out[0, -1].item() in EQUALS_TOKENS:\n",
    "            answer = use_calculator(text)\n",
    "            if answer is not None:\n",
    "                print(\"Triggered calculator, answer\", answer)\n",
    "                text = text + str(answer) + \">>\"\n",
    "\n",
    "        qn = text\n",
    "qn\n",
    "\n",
    "from grade_school_math.dataset import extract_answer, is_correct\n",
    "\n",
    "answer = extract_answer(qn)\n",
    "correct = is_correct(qn, example)\n",
    "qn, answer, correct\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

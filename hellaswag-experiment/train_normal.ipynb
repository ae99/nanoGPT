{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained GPTNeoX: EleutherAI/pythia-70m\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "number of parameters: 70.43M\n"
     ]
    }
   ],
   "source": [
    "# Init modeo\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('./train_normal.ipynb').resolve().parent.parent))\n",
    "\n",
    "from model import GPT\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "model = GPT.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer.add_tokens(['<|dense|>'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dense_token_id = tokenizer.encode('<|dense|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from load_data import train, val, process_example, example_to_text\n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenized_data = []\n",
    "    for example in data:\n",
    "        processed_example = process_example(example)\n",
    "        example_text = example_to_text(processed_example)\n",
    "        \n",
    "        tokens = tokenizer.encode(example_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        tokenized_data.append({\n",
    "            'tokens': tokens,\n",
    "        })\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "train_data = tokenize_data(train[:16*4])\n",
    "val_data = tokenize_data(val[:16*2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HellaSwagDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': item['tokens'],\n",
    "            'length': item['tokens'].shape[1],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = HellaSwagDataset(train_data)\n",
    "val_dataset = HellaSwagDataset(val_data)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, return_tensors='pt')['input_ids']\n",
    "\n",
    "    label_index = torch.tensor([item['length'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids.contiguous(),\n",
    "        'label_index': label_index.contiguous(),\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# we're going to end up with a function that can show both validation and training accuracy too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________\n",
      "' 4' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 3' -> '\\n'\n",
      "' 3' -> '\\n'\n",
      "' 2' -> '\\n'\n",
      "' 2' -> '\\n'\n",
      "' 3' -> '\\n'\n",
      "' 1' -> ' A'\n",
      "' 2' -> '\\n'\n",
      "' 2' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 3' -> '\\n'\n",
      "' 3' -> '\\n'\n",
      "' 1' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "_________________\n",
      "' 3' -> '\\n'\n",
      "' 1' -> ' A'\n",
      "' 2' -> ' A'\n",
      "' 2' -> '\\n'\n",
      "' 2' -> '\\n'\n",
      "' 1' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 1' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 1' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 2' -> '\\n'\n",
      "' 4' -> '\\n'\n",
      "' 2' -> '\\n'\n",
      "' 1' -> '\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        X = batch['input_ids'][:, :-1]\n",
    "        Y = batch['input_ids'][:, 1:]\n",
    "        \n",
    "        # check if Y is contiguous        \n",
    "        noop_dense = torch.zeros((X.shape[0], X.shape[1], model.config.n_embd))\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits, dense, loss = model(X, noop_dense, Y)\n",
    "            \n",
    "            label_index = batch['label_index'] -2\n",
    "            \n",
    "            print(\"_________________\")\n",
    "            \n",
    "            for i in range(Y.shape[0]):\n",
    "                expected = tokenizer.decode(Y[i][label_index[i]])\n",
    "                recieved = tokenizer.decode(logits[i][label_index[i]].argmax(dim=-1))\n",
    "                \n",
    "                print(expected.__repr__(), \"->\", recieved.__repr__())\n",
    "                \n",
    "                if expected == recieved:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            \n",
    "    return correct / total\n",
    "    \n",
    "get_accuracy(model, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b66dbdb614bfd022b649975f993762ad232399f75e8f30cb9176d485ed5b8487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

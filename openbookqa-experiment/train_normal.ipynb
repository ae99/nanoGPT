{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained GPTNeoX: EleutherAI/pythia-70m\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n",
      "number of parameters: 70.43M\n"
     ]
    }
   ],
   "source": [
    "# Init modeo\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('./train_normal.ipynb').resolve().parent.parent))\n",
    "\n",
    "from model import GPT\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "model = GPT.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained('EleutherAI/pythia-70m')\n",
    "tokenizer.add_tokens(['<|dense|>'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dense_token_id = tokenizer.encode('<|dense|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from load_data import train, val, process_example, example_to_text\n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenized_data = []\n",
    "    for example in data:\n",
    "        processed_example = process_example(example)\n",
    "        example_text = example_to_text(processed_example)\n",
    "        \n",
    "        tokens = tokenizer.encode(example_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        tokenized_data.append({\n",
    "            'tokens': tokens,\n",
    "        })\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "train_data = tokenize_data(train)\n",
    "val_data = tokenize_data(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4957, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val)\n",
    "# 64, 16\n",
    "# 623 times larger\n",
    "# 623 * 4.5s = 2803s = 47 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OpenBookQADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': item['tokens'],\n",
    "            'length': item['tokens'].shape[1],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = OpenBookQADataset(train_data)\n",
    "val_dataset = OpenBookQADataset(val_data)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, return_tensors='pt')['input_ids']\n",
    "\n",
    "    label_index = torch.tensor([item['length'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids.contiguous(),\n",
    "        'label_index': label_index.contiguous(),\n",
    "    }\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# we're going to end up with a function that can show both validation and training accuracy too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(Y, label_index, logits, doPrint=False):\n",
    "    label_index = label_index -2\n",
    "    correct = 0\n",
    "    for i in range(Y.shape[0]):\n",
    "        expected = tokenizer.decode(Y[i][label_index[i]])\n",
    "        recieved = tokenizer.decode(logits[i][label_index[i]].argmax(dim=-1))\n",
    "        \n",
    "        if doPrint:\n",
    "            print(expected.__repr__(), \"->\", recieved.__repr__())\n",
    "                    \n",
    "        if expected == recieved:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n",
      "Training Epoch 0\n",
      "Train Batch ------  0 / 309 Accuracy: 0 / 16 = 0.0 Loss: 5.53076696395874\n",
      "Validation Accuracy: 7 / 500 = 0.014 Loss: 2.6390290930867195\n",
      "Train Batch ------  10 / 309 Accuracy: 37 / 160 = 0.23125 Loss: 2.111516237258911\n",
      "Train Batch ------  20 / 309 Accuracy: 34 / 160 = 0.2125 Loss: 2.174377679824829\n",
      "Train Batch ------  30 / 309 Accuracy: 38 / 160 = 0.2375 Loss: 1.8956369161605835\n",
      "Train Batch ------  40 / 309 Accuracy: 33 / 160 = 0.20625 Loss: 1.9248862266540527\n",
      "Train Batch ------  50 / 309 Accuracy: 44 / 160 = 0.275 Loss: 1.7772445678710938\n",
      "Validation Accuracy: 144 / 500 = 0.288 Loss: 1.8365764059126377\n",
      "Train Batch ------  60 / 309 Accuracy: 42 / 160 = 0.2625 Loss: 1.958853840827942\n",
      "Train Batch ------  70 / 309 Accuracy: 42 / 160 = 0.2625 Loss: 2.0282864570617676\n",
      "Train Batch ------  80 / 309 Accuracy: 27 / 160 = 0.16875 Loss: 1.7193377017974854\n",
      "Train Batch ------  90 / 309 Accuracy: 45 / 160 = 0.28125 Loss: 1.5906317234039307\n",
      "Train Batch ------  100 / 309 Accuracy: 30 / 160 = 0.1875 Loss: 2.08174467086792\n",
      "Validation Accuracy: 111 / 500 = 0.222 Loss: 1.7945767417550087\n",
      "Train Batch ------  110 / 309 Accuracy: 40 / 160 = 0.25 Loss: 2.0044987201690674\n",
      "Train Batch ------  120 / 309 Accuracy: 53 / 160 = 0.33125 Loss: 1.5645148754119873\n",
      "Train Batch ------  130 / 309 Accuracy: 46 / 160 = 0.2875 Loss: 1.877131462097168\n",
      "Train Batch ------  140 / 309 Accuracy: 42 / 160 = 0.2625 Loss: 1.5802574157714844\n",
      "Train Batch ------  150 / 309 Accuracy: 36 / 160 = 0.225 Loss: 1.9006575345993042\n",
      "Validation Accuracy: 123 / 500 = 0.246 Loss: 1.7668801918625832\n",
      "Train Batch ------  160 / 309 Accuracy: 39 / 160 = 0.24375 Loss: 1.9397335052490234\n",
      "Train Batch ------  170 / 309 Accuracy: 35 / 160 = 0.21875 Loss: 1.6323437690734863\n",
      "Train Batch ------  180 / 309 Accuracy: 40 / 160 = 0.25 Loss: 1.8492021560668945\n",
      "Train Batch ------  190 / 309 Accuracy: 49 / 160 = 0.30625 Loss: 1.8286463022232056\n",
      "Train Batch ------  200 / 309 Accuracy: 30 / 160 = 0.1875 Loss: 1.8577792644500732\n",
      "Validation Accuracy: 137 / 500 = 0.274 Loss: 1.74487379565835\n",
      "Train Batch ------  210 / 309 Accuracy: 44 / 160 = 0.275 Loss: 1.5934317111968994\n",
      "Train Batch ------  220 / 309 Accuracy: 45 / 160 = 0.28125 Loss: 1.7676335573196411\n",
      "Train Batch ------  230 / 309 Accuracy: 36 / 160 = 0.225 Loss: 1.5097236633300781\n",
      "Train Batch ------  240 / 309 Accuracy: 40 / 160 = 0.25 Loss: 1.390400767326355\n",
      "Train Batch ------  250 / 309 Accuracy: 36 / 160 = 0.225 Loss: 1.4042274951934814\n",
      "Validation Accuracy: 134 / 500 = 0.268 Loss: 1.7308251336216927\n",
      "Train Batch ------  260 / 309 Accuracy: 38 / 160 = 0.2375 Loss: 1.6607223749160767\n",
      "Train Batch ------  270 / 309 Accuracy: 42 / 160 = 0.2625 Loss: 1.9590712785720825\n",
      "Train Batch ------  280 / 309 Accuracy: 37 / 160 = 0.23125 Loss: 1.8189548254013062\n",
      "Train Batch ------  290 / 309 Accuracy: 33 / 160 = 0.20625 Loss: 1.3497190475463867\n",
      "Train Batch ------  300 / 309 Accuracy: 27 / 160 = 0.16875 Loss: 1.5610798597335815\n",
      "Validation Accuracy: 137 / 500 = 0.274 Loss: 1.7237655222415924\n",
      "Validation Accuracy: 117 / 500 = 0.234 Loss: 1.722436860203743\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-2\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "learning_rate = 5e-5 # max learning rate\n",
    "device_type = 'cpu'\n",
    "epochs = 1\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "\n",
    "model.to(device_type)\n",
    "\n",
    "num_batches = len(train) // batch_size\n",
    "\n",
    "def eval():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    losses = 0\n",
    "    loss_count = 0\n",
    "    \n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        X = batch['input_ids'][:, :-1].to(device_type)\n",
    "        Y = batch['input_ids'][:, 1:].to(device_type)\n",
    "\n",
    "        # check if Y is contiguous        \n",
    "        noop_dense = torch.zeros((X.shape[0], X.shape[1], model.config.n_embd)).to(device_type)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, dense, loss = model(X, noop_dense, Y)\n",
    "\n",
    "            total += X.shape[0]\n",
    "            correct += get_accuracy(Y, batch['label_index'], logits)\n",
    "\n",
    "            losses += loss.item()\n",
    "            loss_count += 1\n",
    "            \n",
    "    model.train()\n",
    "    print(\"Validation Accuracy:\", correct, \"/\", total, \"=\", correct/total, \"Loss:\", losses/loss_count)   \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TRAIN\n",
    "    print(\"Training Epoch\", epoch)\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        X = batch['input_ids'][:, :-1].to(device_type)\n",
    "        Y = batch['input_ids'][:, 1:].to(device_type)\n",
    "        \n",
    "        noop_dense = torch.zeros((X.shape[0], X.shape[1], model.config.n_embd)).to(device_type)\n",
    "\n",
    "        logits, dense, loss = model(X, noop_dense, Y)\n",
    "        \n",
    "        total += X.shape[0]\n",
    "        correct += get_accuracy(Y, batch['label_index'], logits)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Train Batch ------ ', i, '/', num_batches, 'Accuracy:', correct, \"/\", total, '=', correct/total, 'Loss:', loss.item())\n",
    "            correct = 0\n",
    "            total = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            eval()\n",
    "eval()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b66dbdb614bfd022b649975f993762ad232399f75e8f30cb9176d485ed5b8487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

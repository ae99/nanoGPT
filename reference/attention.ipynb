{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\")\n",
    "from modeling_gpt_neox import GPTNeoXAttention\n",
    "attention = GPTNeoXAttention(config=model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "for k, v in model.state_dict().items():\n",
    "    if k.startswith(\"gpt_neox.layers.1.attention.\"):\n",
    "        new_state_dict[k.replace(\"gpt_neox.layers.1.attention.\", \"\")] = v\n",
    "\n",
    "attention.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-6.9567e-03,  3.3929e-03,  1.0490e-02,  ..., -8.9337e-03,\n",
       "            1.2362e-02,  1.7269e-03],\n",
       "          [ 1.2703e-02,  1.9357e-03,  7.1906e-03,  ..., -2.1879e-03,\n",
       "            1.2637e-02, -8.9477e-04],\n",
       "          [ 5.2897e-03,  4.8720e-03,  3.7390e-03,  ...,  1.5283e-04,\n",
       "            1.4059e-02, -3.1914e-03],\n",
       "          [ 2.1649e-03,  5.0285e-03,  3.8143e-03,  ...,  2.2147e-03,\n",
       "            1.0520e-02, -2.4457e-03],\n",
       "          [ 2.3755e-03,  3.5694e-03,  4.6525e-03,  ..., -1.5534e-03,\n",
       "            3.6072e-03, -1.9443e-03]],\n",
       " \n",
       "         [[-2.9194e-03,  7.6916e-03,  4.2001e-03,  ...,  8.7902e-03,\n",
       "            2.7098e-02, -1.8283e-03],\n",
       "          [-1.5174e-03, -3.0349e-03, -1.1576e-02,  ...,  9.2955e-03,\n",
       "            1.7590e-02, -3.2332e-03],\n",
       "          [ 3.4629e-03, -2.9270e-03, -1.1274e-02,  ...,  1.0710e-02,\n",
       "            6.0601e-03,  9.1808e-04],\n",
       "          [ 2.9374e-03, -6.2745e-03, -1.2581e-02,  ...,  1.0311e-02,\n",
       "            2.1893e-03, -1.6492e-03],\n",
       "          [-7.6566e-04,  2.4798e-03, -1.2961e-02,  ...,  8.8321e-03,\n",
       "           -2.3325e-03, -1.3151e-03]],\n",
       " \n",
       "         [[-1.2544e-02, -8.5568e-04, -3.9600e-03,  ..., -2.6167e-03,\n",
       "            1.4065e-02, -1.0782e-03],\n",
       "          [-1.4169e-02,  3.8215e-03, -2.2425e-03,  ..., -1.7587e-03,\n",
       "            1.5967e-02,  7.4093e-05],\n",
       "          [-4.9833e-03,  2.0949e-03, -7.1227e-04,  ...,  2.6593e-03,\n",
       "            1.1033e-02, -4.5756e-04],\n",
       "          [-1.8788e-03,  9.1310e-04,  1.1801e-03,  ..., -8.2351e-05,\n",
       "            1.2658e-02, -2.2209e-03],\n",
       "          [-1.0598e-03,  3.9549e-03,  1.4569e-03,  ..., -4.5316e-04,\n",
       "            7.2621e-03, -1.8114e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-2.0279e-03, -2.0647e-02, -8.7192e-03,  ..., -2.0150e-02,\n",
       "           -7.8931e-03,  1.7196e-02],\n",
       "          [ 7.4679e-03, -1.4236e-02, -8.9918e-03,  ...,  2.9530e-03,\n",
       "           -9.8059e-04,  1.6489e-02],\n",
       "          [ 3.6507e-03, -6.5302e-03, -8.1531e-03,  ..., -2.9352e-05,\n",
       "           -8.5525e-03,  1.2767e-02],\n",
       "          [ 2.6931e-03, -2.8128e-03, -5.1087e-03,  ..., -1.6767e-03,\n",
       "           -2.8333e-03,  9.0105e-03],\n",
       "          [ 1.9711e-03, -8.2495e-04, -5.9468e-03,  ...,  3.0221e-03,\n",
       "           -3.5434e-03,  6.4344e-03]],\n",
       " \n",
       "         [[ 9.2310e-03,  1.0101e-03,  1.2575e-02,  ...,  1.6144e-02,\n",
       "            6.8443e-03, -4.1196e-03],\n",
       "          [ 3.6418e-03, -5.2468e-03,  3.1172e-03,  ...,  1.4234e-02,\n",
       "            1.1855e-02, -1.7697e-03],\n",
       "          [-3.7081e-04,  1.9268e-03,  3.0925e-04,  ...,  1.5168e-02,\n",
       "            1.8227e-03, -2.9010e-03],\n",
       "          [ 3.6825e-04,  6.7246e-03,  2.5061e-03,  ...,  1.2648e-02,\n",
       "            1.1544e-03, -1.2501e-04],\n",
       "          [ 4.5030e-04,  1.6188e-03,  4.3886e-03,  ...,  1.1017e-02,\n",
       "            4.0690e-03, -4.9069e-04]],\n",
       " \n",
       "         [[ 1.2299e-02,  2.2242e-02,  6.0319e-03,  ...,  8.2297e-03,\n",
       "           -1.1532e-02, -1.6701e-03],\n",
       "          [-3.4191e-03,  3.7757e-03, -1.1509e-03,  ...,  1.2042e-02,\n",
       "            2.3572e-03, -1.7539e-02],\n",
       "          [ 2.3818e-04,  9.7011e-04, -4.8465e-03,  ...,  1.3928e-02,\n",
       "           -1.3478e-03, -1.3423e-02],\n",
       "          [ 3.4655e-03, -1.5115e-03, -2.3474e-03,  ...,  1.4150e-02,\n",
       "           -1.4658e-03, -6.0644e-03],\n",
       "          [ 7.0412e-03,  1.2749e-03,  7.8255e-04,  ...,  1.3722e-02,\n",
       "            2.3419e-03, -3.4272e-03]]], grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputState = torch.randn(9, 5, model.config.hidden_size)\n",
    "\n",
    "attention(inputState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "4\n",
      "512\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(model.config.hidden_size)\n",
    "print(model.config.num_attention_heads)\n",
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.hidden_size // model.config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=True)#config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True) #n_embd = hidden_size?\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.n_head = config.num_attention_heads #config.n_head\n",
    "        self.n_embd = config.hidden_size\n",
    "        self.dropout = 0.0\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch nightly and still a bit scary\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and self.dropout == 0.0\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings))\n",
    "                                        .view(1, 1, config.max_position_embeddings, config.max_position_embeddings)) #block_size? -> am I using this wrong?\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        head_size = C // self.n_head\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        \n",
    "        qkv = qkv.view(B, T, self.n_head, 3 * head_size)\n",
    "        q, k, v = qkv.split(head_size, dim=3)\n",
    "        \n",
    "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # TODO: Alexs addition -> works for 1,1 -> now figure out how to make it work for BIGGER\n",
    "        # q, k ,v  = qkv.view(B,T,4,3,8).transpose(0,3)#qkv.split(self.n_embd, dim=2)#\n",
    "        # print(q.size()) #SAME\n",
    "        # print('q1', q)\n",
    "        # print('k', k)\n",
    "        # # print(q.shape, k.shape, v.shape)\n",
    "        # <OLD>\n",
    "        # q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        # k = k.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # q = q.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # v = v.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # print('k', k)\n",
    "        # print('qkv1', qkv)\n",
    "        # </OLD>\n",
    "        \n",
    "        # <NEW>\n",
    "        \n",
    "        # qkv = qkv.view(B, T, self.n_head, 3 * head_size)\n",
    "        \n",
    "        # # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
    "        # q = qkv[..., : head_size].transpose(1, 2)\n",
    "        # k = qkv[..., head_size : 2 * head_size].transpose(1, 2)\n",
    "        # v = qkv[..., 2 * head_size :].transpose(1, 2)\n",
    "        # print('q2', q)\n",
    "        # </NEW>\n",
    "        # <NEWNEW>\n",
    "        # qkv = qkv.view(B, T, self.n_head, 3 * head_size)\n",
    "        # q, k, v = qkv.split(head_size, dim=-1)\n",
    "        # q = q.transpose(1, 2)\n",
    "        # k = k.transpose(1, 2)\n",
    "        # v = v.transpose(1, 2)\n",
    "        # </NEWNEW>\n",
    "        # <NEWNEWNEW>\n",
    "        qkv = qkv.view(B, T, self.n_head, 3 * head_size)\n",
    "        q, k, v = qkv.split(head_size, dim=3)\n",
    "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        # </NEWNEWNEW>\n",
    "        \n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        # print(q[0][0][0][1], k[0][0][0][1], v[0][0][0][1])\n",
    "        # print(q[0][0][0][0], k[0][0][0][0], v[0][0][0][0])\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # print(y.shape)\n",
    "        # output projection\n",
    "        # print(y[0][0][0])\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "attention2 = CausalSelfAttention(model.config)\n",
    "new_state_dict = {\n",
    "    \"c_proj.weight\": model.state_dict()[\"gpt_neox.layers.1.attention.dense.weight\"],\n",
    "    \"c_proj.bias\": model.state_dict()[\"gpt_neox.layers.1.attention.dense.bias\"],\n",
    "    \"c_attn.weight\": model.state_dict()[\"gpt_neox.layers.1.attention.query_key_value.weight\"],\n",
    "    \"c_attn.bias\": model.state_dict()[\"gpt_neox.layers.1.attention.query_key_value.bias\"],\n",
    "    \"bias\": model.state_dict()[\"gpt_neox.layers.1.attention.bias\"],\n",
    "}\n",
    "# print([(k, v.size()) for k,v in attention2.state_dict().items()])\n",
    "# print([(k, v.size()) for k,v in new_state_dict.items()])\n",
    "\n",
    "attention2.load_state_dict(new_state_dict)\n",
    "\n",
    "from modeling_gpt import GPT2Attention\n",
    "gpt2Attention = GPT2Attention(model.config)\n",
    "\n",
    "new_state_dict['c_attn.weight'] = new_state_dict['c_attn.weight'].t()\n",
    "new_state_dict['c_proj.weight'] = new_state_dict['c_proj.weight'].t()\n",
    "new_state_dict['masked_bias'] = gpt2Attention.state_dict()['masked_bias']\n",
    "gpt2Attention.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "x = attention2(inputState)\n",
    "y = attention(inputState)[0]\n",
    "torch.allclose(x,y)\n",
    "# z = gpt2Attention(inputState)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0066, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0002, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x[0][0][0], y[0][0][0]#, z[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUCCESS\n",
    "It worked, we figured it out!\n",
    "\n",
    "So, the *two* differneces between the self attentions are:\n",
    "1. How they each unpack the `qkv`'s\n",
    "2. Addition of rotary embeddings\n",
    "\n",
    "It is *completely* unclear to me why they each do these different variations for the unpacking. If I'm lucky I might be able to figure out how to reshape things to get the same results.\n",
    "\n",
    "I think that's the next test I should do in that case... isolate out each of these pieces and try to decipher how they manipulate the various weights\n",
    "If I can end up minimising the difference between them, then *fuck yes*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b66dbdb614bfd022b649975f993762ad232399f75e8f30cb9176d485ed5b8487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

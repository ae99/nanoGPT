{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\")\n",
    "from modeling_gpt_neox import GPTNeoXAttention\n",
    "attention = GPTNeoXAttention(config=model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "for k, v in model.state_dict().items():\n",
    "    if k.startswith(\"gpt_neox.layers.1.attention.\"):\n",
    "        new_state_dict[k.replace(\"gpt_neox.layers.1.attention.\", \"\")] = v\n",
    "\n",
    "attention.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.4565e-02,  1.0580e-03,  2.1217e-03,  ..., -1.8174e-03,\n",
       "            1.3156e-02,  1.6827e-03],\n",
       "          [ 1.4614e-03,  5.8233e-03,  3.3597e-03,  ..., -2.6782e-03,\n",
       "            1.2068e-02,  5.5921e-03],\n",
       "          [ 7.4161e-03, -6.1869e-03, -3.0464e-03,  ..., -5.6295e-03,\n",
       "            3.9184e-03,  1.0624e-02],\n",
       "          [ 1.1426e-02, -5.5984e-03, -2.1029e-03,  ..., -5.5380e-03,\n",
       "            1.0034e-03,  9.9084e-03],\n",
       "          [ 1.0451e-02, -1.0159e-02, -4.2054e-03,  ..., -1.1213e-03,\n",
       "           -1.1776e-03,  7.0760e-03]],\n",
       " \n",
       "         [[ 8.6722e-04, -7.8111e-03, -3.4572e-04,  ..., -9.0739e-03,\n",
       "            1.0469e-02,  8.3143e-04],\n",
       "          [ 7.0545e-03, -1.0884e-02,  8.1043e-03,  ..., -6.9769e-03,\n",
       "            5.4900e-03,  7.9795e-03],\n",
       "          [-1.1418e-05, -4.8175e-03,  8.2175e-03,  ..., -5.4572e-03,\n",
       "            1.3758e-02,  6.2788e-03],\n",
       "          [-1.1667e-03, -8.8485e-03,  9.1316e-03,  ..., -5.6465e-03,\n",
       "            4.1825e-03,  3.7440e-03],\n",
       "          [ 5.2492e-04, -1.0774e-02,  3.3510e-03,  ..., -5.6989e-03,\n",
       "            2.5280e-03,  4.2429e-03]],\n",
       " \n",
       "         [[-2.2434e-02,  1.2504e-02,  2.4466e-03,  ..., -1.2337e-02,\n",
       "            9.0005e-03, -4.9692e-03],\n",
       "          [-7.6860e-04, -4.0434e-03, -5.0708e-03,  ...,  3.9535e-03,\n",
       "            1.0278e-02, -3.6230e-04],\n",
       "          [ 1.3612e-03, -2.1591e-03, -5.4836e-03,  ...,  4.3983e-03,\n",
       "            1.3926e-02, -1.2422e-04],\n",
       "          [-1.5560e-03, -9.3982e-04, -1.8746e-04,  ...,  5.7710e-03,\n",
       "            1.0791e-02,  1.7693e-03],\n",
       "          [ 8.9000e-04,  7.6999e-04,  1.0217e-03,  ...,  6.7782e-03,\n",
       "            2.0085e-03,  7.9242e-04]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 8.0903e-03, -2.2443e-03, -1.0722e-03,  ..., -2.8356e-02,\n",
       "            1.1179e-02,  1.9791e-02],\n",
       "          [ 4.0754e-03, -4.7038e-03,  4.7616e-03,  ..., -1.3199e-02,\n",
       "            1.3958e-02,  8.5599e-03],\n",
       "          [ 7.2996e-03, -5.5828e-03,  3.9132e-03,  ..., -5.4410e-03,\n",
       "            1.2204e-02,  5.9176e-03],\n",
       "          [ 5.0094e-03, -4.4944e-03,  6.8648e-03,  ..., -1.0133e-02,\n",
       "            1.4266e-02,  7.5645e-03],\n",
       "          [ 3.3355e-03, -1.3358e-03,  7.2507e-03,  ..., -3.9963e-03,\n",
       "            1.7149e-02,  4.1608e-03]],\n",
       " \n",
       "         [[ 3.2567e-03,  9.9700e-03,  7.2898e-03,  ...,  1.4737e-03,\n",
       "            1.7514e-02,  9.5471e-03],\n",
       "          [ 6.7582e-03, -9.6811e-04,  8.7209e-03,  ..., -7.9382e-03,\n",
       "            1.3898e-02,  1.5464e-02],\n",
       "          [ 4.8565e-03, -4.9787e-03,  4.2259e-03,  ..., -4.7199e-03,\n",
       "            1.2395e-02,  7.5209e-03],\n",
       "          [ 1.3834e-04, -6.6991e-03,  2.9654e-03,  ..., -2.3679e-03,\n",
       "            6.7311e-03,  2.0211e-03],\n",
       "          [-4.3867e-03, -1.9715e-03, -6.2160e-05,  ..., -4.6033e-03,\n",
       "            1.1342e-04,  1.3997e-03]],\n",
       " \n",
       "         [[ 1.3253e-02, -4.3872e-03, -5.1596e-03,  ..., -6.2321e-03,\n",
       "            2.0981e-02,  1.0212e-02],\n",
       "          [ 1.6268e-02, -7.1247e-04, -5.2601e-03,  ..., -1.0835e-02,\n",
       "           -6.1523e-03,  1.4289e-02],\n",
       "          [ 1.3282e-02,  2.9381e-03, -1.5168e-03,  ..., -8.7044e-03,\n",
       "           -1.2740e-02,  1.2245e-02],\n",
       "          [ 1.2518e-02,  6.9095e-04,  8.8646e-04,  ..., -8.0872e-03,\n",
       "           -9.1848e-03,  1.1473e-02],\n",
       "          [ 1.0390e-02,  2.5346e-04,  3.2543e-03,  ..., -5.7245e-03,\n",
       "           -6.8671e-04,  8.9939e-03]]], grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputState = torch.randn(9, 5, model.config.hidden_size)\n",
    "\n",
    "attention(inputState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "4\n",
      "512\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(model.config.hidden_size)\n",
    "print(model.config.num_attention_heads)\n",
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.hidden_size // model.config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=True)#config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True) #n_embd = hidden_size?\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.n_head = config.num_attention_heads #config.n_head\n",
    "        self.n_embd = config.hidden_size\n",
    "        self.dropout = 0.0\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch nightly and still a bit scary\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and self.dropout == 0.0\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim = int((config.hidden_size // config.num_attention_heads) * config.rotary_pct)\n",
    "        )\n",
    "        \n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings))\n",
    "                                        .view(1, 1, config.max_position_embeddings, config.max_position_embeddings)) #block_size? -> am I using this wrong?\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        head_size = C // self.n_head\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        \n",
    "        # GPT has things arranged as k,k,k,k,v,v,v,v,q,q,q,q whereas NeoX has them as k,q,v,k,q,v,k,q,v,k,q,v\n",
    "        qkv = qkv.view(B, T, self.n_head, 3 * head_size)\n",
    "        q, k, v = qkv.split(head_size, dim=3)\n",
    "        \n",
    "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "attention2 = CausalSelfAttention(model.config)\n",
    "new_state_dict = {\n",
    "    \"c_proj.weight\": model.state_dict()[\"gpt_neox.layers.1.attention.dense.weight\"],\n",
    "    \"c_proj.bias\": model.state_dict()[\"gpt_neox.layers.1.attention.dense.bias\"],\n",
    "    \"c_attn.weight\": model.state_dict()[\"gpt_neox.layers.1.attention.query_key_value.weight\"],\n",
    "    \"c_attn.bias\": model.state_dict()[\"gpt_neox.layers.1.attention.query_key_value.bias\"],\n",
    "    \"bias\": model.state_dict()[\"gpt_neox.layers.1.attention.bias\"],\n",
    "    \"rotary_emb.freqs\": attention2.state_dict()[\"rotary_emb.freqs\"]\n",
    "}\n",
    "# print([(k, v.size()) for k,v in attention2.state_dict().items()])\n",
    "# print([(k, v.size()) for k,v in new_state_dict.items()])\n",
    "\n",
    "attention2.load_state_dict(new_state_dict)\n",
    "\n",
    "x = attention2(inputState)\n",
    "y = attention(inputState)[0]\n",
    "torch.allclose(x,y)\n",
    "# z = gpt2Attention(inputState)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0246, grad_fn=<SelectBackward0>),\n",
       " tensor(-0.0246, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x[0][0][0], y[0][0][0]#, z[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUCCESS\n",
    "It worked, we figured it out!\n",
    "\n",
    "So, the *two* differneces between the self attentions are:\n",
    "1. How they each unpack the `qkv`'s\n",
    "2. Addition of rotary embeddings\n",
    "\n",
    "It is *completely* unclear to me why they each do these different variations for the unpacking. If I'm lucky I might be able to figure out how to reshape things to get the same results.\n",
    "\n",
    "I think that's the next test I should do in that case... isolate out each of these pieces and try to decipher how they manipulate the various weights\n",
    "If I can end up minimising the difference between them, then *fuck yes*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mseq_len \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is larger than max_position_embeddings \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(seq_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_len_cached))\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcos_cached[:seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 41\u001b[0m query_rot \u001b[39m=\u001b[39m query[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_ndims]\n\u001b[1;32m     42\u001b[0m query_pass \u001b[39m=\u001b[39m query[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_ndims :]\n\u001b[1;32m     43\u001b[0m key_rot \u001b[39m=\u001b[39m key[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_ndims]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "# Now time for rotary embeddings\n",
    "\n",
    "# Seems like https://github.com/lucidrains/rotary-embedding-torch/blob/main/README.md is a clean implementation\n",
    "# First step is to pull it in and see if it works\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n",
    "    cos = cos[..., offset : q.shape[-2] + offset, :]\n",
    "    sin = sin[..., offset : q.shape[-2] + offset, :]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = emb.cos()[None, None, :, :]\n",
    "        self.sin_cached = emb.sin()[None, None, :, :]\n",
    "\n",
    "    # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "    def forward(self, x, seq_len=None):\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            raise ValueError(\"seq_len {} is larger than max_position_embeddings {}\".format(seq_len, self.max_seq_len_cached))\n",
    "\n",
    "        return self.cos_cached[:seq_len, ...].to(x.device), self.sin_cached[:seq_len, ...].to(x.device)\n",
    "\n",
    "query_rot = query[..., : self.rotary_ndims]\n",
    "query_pass = query[..., self.rotary_ndims :]\n",
    "key_rot = key[..., : self.rotary_ndims]\n",
    "key_pass = key[..., self.rotary_ndims :]\n",
    "seq_len = key.shape[-2]\n",
    "cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
    "\n",
    "query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, offset=offset)\n",
    "\n",
    "query = torch.cat((query, query_pass), dim=-1)\n",
    "key = torch.cat((key, key_pass), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rotary-embedding-torch\n",
      "  Downloading rotary_embedding_torch-0.2.1-py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/alex/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from rotary-embedding-torch) (1.13.1)\n",
      "Collecting einops>=0.3\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/alex/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch>=1.6->rotary-embedding-torch) (4.3.0)\n",
      "Installing collected packages: einops, rotary-embedding-torch\n",
      "Successfully installed einops-0.6.0 rotary-embedding-torch-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rotary-embedding-torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b66dbdb614bfd022b649975f993762ad232399f75e8f30cb9176d485ed5b8487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

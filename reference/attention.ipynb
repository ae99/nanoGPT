{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"trl-internal-testing/tiny-random-GPTNeoXForCausalLM\")\n",
    "from modeling_gpt_neox import GPTNeoXAttention\n",
    "attention = GPTNeoXAttention(config=model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "for k, v in model.state_dict().items():\n",
    "    if k.startswith(\"gpt_neox.layers.1.attention.\"):\n",
    "        new_state_dict[k.replace(\"gpt_neox.layers.1.attention.\", \"\")] = v\n",
    "\n",
    "attention.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.8310e-02, -1.6087e-01, -2.0428e-01, -2.4702e-02, -2.0972e-01,\n",
      "           8.7988e-02, -2.6497e-01,  2.7737e-02,  1.2506e-01, -2.6803e-02,\n",
      "          -1.1552e-02,  4.4691e-02, -3.9390e-02, -2.2193e-02,  3.2879e-02,\n",
      "          -5.4363e-02, -8.1543e-02, -1.6463e-01,  1.6081e-02,  3.3943e-02,\n",
      "           4.6513e-03,  2.4262e-02, -3.7914e-02, -2.5462e-01,  1.0213e-01,\n",
      "           1.4505e-01, -4.7030e-02, -1.1044e-01,  3.2030e-04,  6.8774e-02,\n",
      "           7.7158e-03,  3.0727e-02, -1.7373e-02, -1.5557e-01, -2.0885e-01,\n",
      "           7.3585e-02, -3.4552e-02,  1.4937e-01, -1.7941e-01,  2.3737e-01,\n",
      "           6.8656e-02, -2.1996e-01, -1.1606e-01, -1.0428e-01, -5.2216e-02,\n",
      "          -1.0184e-01, -2.4314e-02, -1.8734e-01,  4.6922e-02,  2.3626e-01,\n",
      "           4.5409e-02,  6.1575e-02,  5.7560e-02, -5.6481e-03, -1.2850e-01,\n",
      "           3.5208e-02,  7.2847e-02,  2.0259e-01, -7.0605e-02, -1.2796e-01,\n",
      "          -3.6485e-02,  4.1885e-02, -8.9488e-03, -1.9154e-01,  3.6256e-02,\n",
      "          -2.2605e-02,  4.4694e-02, -8.7330e-02, -3.2510e-01,  4.7686e-02,\n",
      "           8.6739e-02, -7.0080e-02,  7.8402e-02, -8.2809e-02, -4.0134e-02,\n",
      "          -1.7626e-01, -1.1682e-01, -2.6237e-02,  1.7053e-02, -1.0811e-01,\n",
      "           7.7921e-02, -7.9940e-02, -7.1352e-02,  1.8943e-01, -2.3648e-02,\n",
      "           1.6447e-02, -1.9153e-01,  6.0972e-02, -1.5909e-01, -6.1489e-02,\n",
      "          -1.4924e-01, -1.0759e-01, -1.0806e-02,  1.2004e-01, -1.3503e-01,\n",
      "           3.6073e-02],\n",
      "         [-2.8225e-02, -1.1100e-01, -2.5030e-01, -2.7313e-01, -2.0595e-01,\n",
      "           1.7690e-01, -9.0921e-02, -5.1916e-02,  5.9718e-02, -5.9219e-02,\n",
      "           1.4262e-01, -6.8263e-02, -7.8973e-02,  3.3769e-02,  8.2882e-02,\n",
      "           2.2831e-02, -3.8646e-02, -7.8528e-02,  1.5865e-01, -7.7589e-02,\n",
      "          -2.5997e-02,  1.3692e-01,  7.5691e-02, -9.5083e-02,  1.1975e-01,\n",
      "           1.2093e-01, -8.2964e-03,  4.7019e-02, -3.2958e-02, -7.3595e-02,\n",
      "          -4.6806e-02,  2.8599e-02, -2.6309e-02,  5.7319e-02, -1.8194e-02,\n",
      "          -6.1372e-02,  7.2637e-04,  1.2533e-02, -7.2501e-02,  9.0338e-03,\n",
      "          -7.4887e-03,  5.2639e-02,  4.4937e-02, -1.8314e-01, -9.4288e-02,\n",
      "          -1.6867e-02, -3.3435e-02, -2.1991e-01, -5.7083e-02,  2.4311e-02,\n",
      "           1.6657e-02,  1.5061e-01,  1.2996e-01, -1.0255e-02, -2.8007e-01,\n",
      "          -1.4390e-01, -5.0524e-02,  2.3872e-01, -2.6277e-02,  2.9340e-02,\n",
      "           8.5904e-02, -8.8366e-03, -4.4422e-03, -9.9577e-03,  8.9553e-03,\n",
      "           1.5000e-02, -9.9818e-02,  2.9218e-02, -1.0715e-01, -7.0807e-02,\n",
      "          -8.6673e-02, -6.2638e-02, -9.3620e-02, -1.9041e-01, -1.0177e-02,\n",
      "          -2.6868e-02,  5.1712e-02, -6.9810e-02, -7.8175e-02, -8.1094e-03,\n",
      "          -2.5731e-02,  5.2593e-02, -6.4767e-02, -1.5018e-02,  6.5296e-03,\n",
      "          -1.1768e-01, -9.0959e-02, -2.0051e-02, -3.4230e-02, -8.1460e-02,\n",
      "          -1.8025e-02, -3.6229e-02,  2.4453e-02,  8.4257e-02, -2.0887e-01,\n",
      "          -3.9354e-02],\n",
      "         [-2.1558e-02, -9.1665e-02,  2.1530e-01,  5.6428e-02, -2.9136e-02,\n",
      "          -1.7133e-01,  1.1210e-01,  1.4184e-01, -4.3638e-02, -1.4945e-01,\n",
      "          -2.5632e-01, -2.5825e-02, -5.5554e-02, -5.4441e-02, -8.9542e-02,\n",
      "           9.7722e-02,  6.8276e-02, -1.3535e-01, -1.1997e-01,  1.0901e-01,\n",
      "          -4.7915e-02,  7.3567e-02, -1.1443e-01,  1.1315e-01,  9.1948e-02,\n",
      "           1.5436e-01,  1.7965e-01, -6.4862e-02, -1.0676e-01,  1.2773e-01,\n",
      "          -9.0472e-02, -1.6998e-03, -2.1018e-01, -1.1085e-01, -1.8142e-01,\n",
      "           1.2405e-01, -8.1432e-03, -9.8177e-03,  3.7089e-02,  4.8149e-02,\n",
      "           1.1980e-01,  1.4533e-01,  2.3037e-01, -1.4333e-01, -1.0879e-01,\n",
      "           2.5380e-01,  2.4813e-02, -9.9397e-03, -5.5007e-02, -3.8181e-02,\n",
      "          -3.1223e-03, -1.5175e-01, -2.6153e-02, -6.0414e-02,  9.7676e-02,\n",
      "           9.4685e-02,  4.1286e-02,  3.2468e-02, -4.3066e-02,  1.8327e-01,\n",
      "           1.9179e-01,  8.5690e-02, -1.5463e-01, -1.7165e-01, -2.4720e-02,\n",
      "          -8.0862e-02, -1.2501e-01, -4.6440e-02, -1.2728e-01, -5.3049e-02,\n",
      "           1.0766e-01, -1.9510e-02, -1.1402e-01, -3.0420e-02,  2.9798e-02,\n",
      "           2.4794e-01,  1.9066e-02, -9.4091e-02, -5.0897e-02,  7.2750e-02,\n",
      "           3.2658e-03,  1.3434e-02, -2.0679e-01, -2.5543e-01,  8.0830e-02,\n",
      "          -2.4468e-01,  1.3048e-01,  1.1142e-01, -1.9198e-01,  1.9674e-01,\n",
      "           1.1920e-01,  1.1770e-02,  1.2215e-01, -4.5713e-02,  1.0530e-01,\n",
      "           6.2669e-02]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.1251, -0.0268, -0.0116,  0.0447, -0.0394, -0.0222,  0.0329,\n",
      "           -0.0544],\n",
      "          [-0.0174, -0.1556, -0.2089,  0.0736, -0.0346,  0.1494, -0.1794,\n",
      "            0.2374],\n",
      "          [ 0.0728,  0.2026, -0.0706, -0.1280, -0.0365,  0.0419, -0.0089,\n",
      "           -0.1915],\n",
      "          [ 0.0779, -0.0799, -0.0714,  0.1894, -0.0236,  0.0164, -0.1915,\n",
      "            0.0610]],\n",
      "\n",
      "         [[ 0.0597, -0.0592,  0.1426, -0.0683, -0.0790,  0.0338,  0.0829,\n",
      "            0.0228],\n",
      "          [-0.0263,  0.0573, -0.0182, -0.0614,  0.0007,  0.0125, -0.0725,\n",
      "            0.0090],\n",
      "          [-0.0505,  0.2387, -0.0263,  0.0293,  0.0859, -0.0088, -0.0044,\n",
      "           -0.0100],\n",
      "          [-0.0257,  0.0526, -0.0648, -0.0150,  0.0065, -0.1177, -0.0910,\n",
      "           -0.0201]],\n",
      "\n",
      "         [[-0.0436, -0.1494, -0.2563, -0.0258, -0.0556, -0.0544, -0.0895,\n",
      "            0.0977],\n",
      "          [-0.2102, -0.1109, -0.1814,  0.1241, -0.0081, -0.0098,  0.0371,\n",
      "            0.0481],\n",
      "          [ 0.0413,  0.0325, -0.0431,  0.1833,  0.1918,  0.0857, -0.1546,\n",
      "           -0.1716],\n",
      "          [ 0.0033,  0.0134, -0.2068, -0.2554,  0.0808, -0.2447,  0.1305,\n",
      "            0.1114]]]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 8.4380e-04, -1.8707e-02, -7.9106e-03, -2.1717e-02, -1.4750e-04,\n",
       "            1.0221e-02,  9.0379e-03, -1.3679e-02,  3.0044e-03, -3.0853e-03,\n",
       "            1.9691e-03, -5.2051e-03, -4.7137e-03, -1.4293e-03, -1.5147e-03,\n",
       "            2.3709e-02,  5.7025e-03, -1.2336e-02, -3.1603e-03, -2.0023e-02,\n",
       "            2.8193e-02, -1.0411e-02, -1.4372e-02,  3.0220e-02, -8.8097e-03,\n",
       "           -1.3184e-03, -9.6720e-03,  3.7320e-03,  2.0063e-02, -5.3158e-03,\n",
       "           -2.9216e-03,  6.4100e-03],\n",
       "          [ 2.2677e-03, -1.3946e-02, -3.2214e-03, -1.9329e-02,  2.7390e-03,\n",
       "            8.7879e-03,  1.1250e-02, -4.1818e-03,  1.1194e-02, -8.2719e-04,\n",
       "            4.4894e-03, -4.9629e-03,  5.1314e-06,  2.4338e-03,  2.5025e-03,\n",
       "            2.6363e-02,  1.2752e-02, -1.2245e-02, -1.6854e-03, -1.1198e-02,\n",
       "            2.1714e-02, -1.0845e-02, -8.9126e-03,  2.0203e-02, -1.3107e-02,\n",
       "           -1.7376e-03,  1.5407e-03,  6.5900e-03,  2.0691e-02, -1.1573e-02,\n",
       "           -5.4017e-03,  7.8119e-03],\n",
       "          [ 2.2661e-03, -1.3392e-02, -1.7025e-04, -8.3612e-03, -2.7240e-03,\n",
       "            6.2445e-03,  3.5160e-03, -9.4353e-04, -6.1496e-04, -3.2716e-05,\n",
       "            4.9602e-03,  5.7498e-04, -1.3875e-03,  6.8027e-03,  3.9307e-03,\n",
       "            1.8174e-02,  1.2999e-02, -6.0845e-03,  1.1348e-02, -3.4656e-03,\n",
       "            1.2049e-02, -1.6993e-02, -5.5794e-03,  1.6486e-02, -1.0743e-02,\n",
       "            1.8162e-03,  2.6791e-03,  4.7322e-03,  1.6198e-02, -5.6797e-03,\n",
       "           -1.0538e-02,  8.0689e-03]]], grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputState = torch.randn(1, 3, model.config.hidden_size)\n",
    "\n",
    "attention(inputState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "4\n",
      "512\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(model.config.hidden_size)\n",
    "print(model.config.num_attention_heads)\n",
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.hidden_size // model.config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 100\u001b[0m\n\u001b[1;32m     96\u001b[0m new_state_dict[\u001b[39m'\u001b[39m\u001b[39mmasked_bias\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gpt2Attention\u001b[39m.\u001b[39mstate_dict()[\u001b[39m'\u001b[39m\u001b[39mmasked_bias\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     97\u001b[0m gpt2Attention\u001b[39m.\u001b[39mload_state_dict(new_state_dict)\n\u001b[0;32m--> 100\u001b[0m x \u001b[39m=\u001b[39m attention2(inputState)\n\u001b[1;32m    101\u001b[0m y \u001b[39m=\u001b[39m attention(inputState)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mallclose(x,y)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [68], line 34\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m qkv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_attn(x)\n\u001b[1;32m     33\u001b[0m \u001b[39m# print(qkv.size(), qkv[2][5][1]) #SAME\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m q, k ,v  \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mview(\u001b[39m4\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m#qkv.split(self.n_embd, dim=2)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# print('q1', q)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# print('k', k)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# # print(q.shape, k.shape, v.shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(B, T, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head, head_size)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39m# (B, nh, T, hs)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=True)#config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True) #n_embd = hidden_size?\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.n_head = config.num_attention_heads #config.n_head\n",
    "        self.n_embd = config.hidden_size\n",
    "        self.dropout = 0.0\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch nightly and still a bit scary\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and self.dropout == 0.0\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings))\n",
    "                                        .view(1, 1, config.max_position_embeddings, config.max_position_embeddings)) #block_size? -> am I using this wrong?\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        head_size = C // self.n_head\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        # print(qkv.size(), qkv[2][5][1]) #SAME\n",
    "        \n",
    "        # TODO: Alexs addition -> works for 1,1 -> now figure out how to make it work for BIGGER\n",
    "        q, k ,v  = qkv.view(4, -1, 8).transpose(0, 1)#qkv.split(self.n_embd, dim=2)\n",
    "        # print('q1', q)\n",
    "        # print('k', k)\n",
    "        # # print(q.shape, k.shape, v.shape)\n",
    "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # print('k', k)\n",
    "        # print('qkv1', qkv)\n",
    "\n",
    "        # <NEW>\n",
    "        # new_qkv_shape = qkv.size()[:-1] + (self.n_head, 3 * head_size)\n",
    "        # print(new_qkv_shape)\n",
    "        # qkv = qkv.view(*new_qkv_shape)\n",
    "        # print('qkv2', qkv)\n",
    "        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
    "        # q = qkv[..., : head_size].permute(0, 2, 1, 3)\n",
    "        # k = qkv[..., head_size : 2 * head_size].permute(0, 2, 1, 3)\n",
    "        # v = qkv[..., 2 * head_size :].permute(0, 2, 1, 3)\n",
    "        # print('q2', q)\n",
    "        # </NEW>\n",
    "        \n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        # print(q[0][0][0][1], k[0][0][0][1], v[0][0][0][1])\n",
    "        # print(q[0][0][0][0], k[0][0][0][0], v[0][0][0][0])\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # print(y.shape)\n",
    "        # output projection\n",
    "        # print(y[0][0][0])\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "attention2 = CausalSelfAttention(model.config)\n",
    "new_state_dict = {\n",
    "    \"c_proj.weight\": model.state_dict()[\"gpt_neox.layers.1.attention.dense.weight\"],\n",
    "    \"c_proj.bias\": model.state_dict()[\"gpt_neox.layers.1.attention.dense.bias\"],\n",
    "    \"c_attn.weight\": model.state_dict()[\"gpt_neox.layers.1.attention.query_key_value.weight\"],\n",
    "    \"c_attn.bias\": model.state_dict()[\"gpt_neox.layers.1.attention.query_key_value.bias\"],\n",
    "    \"bias\": model.state_dict()[\"gpt_neox.layers.1.attention.bias\"],\n",
    "}\n",
    "# print([(k, v.size()) for k,v in attention2.state_dict().items()])\n",
    "# print([(k, v.size()) for k,v in new_state_dict.items()])\n",
    "\n",
    "attention2.load_state_dict(new_state_dict)\n",
    "\n",
    "from modeling_gpt import GPT2Attention\n",
    "gpt2Attention = GPT2Attention(model.config)\n",
    "\n",
    "new_state_dict['c_attn.weight'] = new_state_dict['c_attn.weight'].t()\n",
    "new_state_dict['c_proj.weight'] = new_state_dict['c_proj.weight'].t()\n",
    "new_state_dict['masked_bias'] = gpt2Attention.state_dict()['masked_bias']\n",
    "gpt2Attention.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "x = attention2(inputState)\n",
    "y = attention(inputState)[0]\n",
    "torch.allclose(x,y)\n",
    "# z = gpt2Attention(inputState)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0068, grad_fn=<SelectBackward0>),\n",
       " tensor(-0.0068, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x[0][0][0], y[0][0][0]#, z[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUCCESS\n",
    "It worked, we figured it out!\n",
    "\n",
    "So, the *two* differneces between the self attentions are:\n",
    "1. How they each unpack the `qkv`'s\n",
    "2. Addition of rotary embeddings\n",
    "\n",
    "It is *completely* unclear to me why they each do these different variations for the unpacking. If I'm lucky I might be able to figure out how to reshape things to get the same results.\n",
    "\n",
    "I think that's the next test I should do in that case... isolate out each of these pieces and try to decipher how they manipulate the various weights\n",
    "If I can end up minimising the difference between them, then *fuck yes*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b66dbdb614bfd022b649975f993762ad232399f75e8f30cb9176d485ed5b8487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
